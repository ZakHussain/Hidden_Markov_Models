{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A2.2 Part-of-Speech Tagging using HMM  \n",
    "**Author:** Zak Hussain  \n",
    "**Weeks:** week 3 & 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Purpose**  \n",
    "The goal for this section is to build a POS tagger using HMM as explained the section 8.4 reading: https://web.stanford.edu/~jurafsky/slp3/8.pdf.  \n",
    "\n",
    "The Training data used comes from the brown corpus. Each file contains sentences of tokenized words followed by POS tags and where each line contains a sentence. Here is the link to the dataset: https://www.dropbox.com/s/m22lg4kucvpv815/brown.zip?dl=0\n",
    "\n",
    "The test set is also from the brown corpus. Each file contains tokenized words, but no tages. Here is the link to the dataset: https://www.dropbox.com/s/bg0cfdd2igjrmyl/Test_File.txt?dl=0\n",
    "\n",
    "Implement a POS tagger using a bigram HMM. Given an observed sequence of n words w1, w2, ..wn, choose the most probable sequence of POS tags  t1, t2,...tn. \n",
    "\n",
    "**NOTE:** during training, choose a *frequency threshold* (e.g., 5) and trat any words that have freq below this as unknown. Use observations to select your observations. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Tasks:**\n",
    "* 2.1 - Frequency Counts \n",
    "* 2.2 - Transition Probabilities \n",
    "* 2.3 - Emission Probabilities \n",
    "* 2.4 - Viterbi Algorithm "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 - Frequency Counts:   \n",
    "Obtain Frequency Counts from the training set. Gather Tag-word pair counts, Tag-unigram counts, Tag-bigram counts."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step 0.a.**  \n",
    "create list that contains tokenized sentences across the entire corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is what the first two elements in the list looks like--each list is a tokenized sentence.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[['The/at',\n",
       "  'Fulton/np-tl',\n",
       "  'County/nn-tl',\n",
       "  'Grand/jj-tl',\n",
       "  'Jury/nn-tl',\n",
       "  'said/vbd',\n",
       "  'Friday/nr',\n",
       "  'an/at',\n",
       "  'investigation/nn',\n",
       "  'of/in',\n",
       "  \"Atlanta's/np$\",\n",
       "  'recent/jj',\n",
       "  'primary/nn',\n",
       "  'election/nn',\n",
       "  'produced/vbd',\n",
       "  '``/``',\n",
       "  'no/at',\n",
       "  'evidence/nn',\n",
       "  \"''/''\",\n",
       "  'that/cs',\n",
       "  'any/dti',\n",
       "  'irregularities/nns',\n",
       "  'took/vbd',\n",
       "  'place/nn',\n",
       "  './.'],\n",
       " ['The/at',\n",
       "  'jury/nn',\n",
       "  'further/rbr',\n",
       "  'said/vbd',\n",
       "  'in/in',\n",
       "  'term-end/nn',\n",
       "  'presentments/nns',\n",
       "  'that/cs',\n",
       "  'the/at',\n",
       "  'City/nn-tl',\n",
       "  'Executive/jj-tl',\n",
       "  'Committee/nn-tl',\n",
       "  ',/,',\n",
       "  'which/wdt',\n",
       "  'had/hvd',\n",
       "  'over-all/jj',\n",
       "  'charge/nn',\n",
       "  'of/in',\n",
       "  'the/at',\n",
       "  'election/nn',\n",
       "  ',/,',\n",
       "  '``/``',\n",
       "  'deserves/vbz',\n",
       "  'the/at',\n",
       "  'praise/nn',\n",
       "  'and/cc',\n",
       "  'thanks/nns',\n",
       "  'of/in',\n",
       "  'the/at',\n",
       "  'City/nn-tl',\n",
       "  'of/in-tl',\n",
       "  'Atlanta/np-tl',\n",
       "  \"''/''\",\n",
       "  'for/in',\n",
       "  'the/at',\n",
       "  'manner/nn',\n",
       "  'in/in',\n",
       "  'which/wdt',\n",
       "  'the/at',\n",
       "  'election/nn',\n",
       "  'was/bedz',\n",
       "  'conducted/vbn',\n",
       "  './.']]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get the training data\n",
    "from nltk.util import ngrams \n",
    "import os \n",
    "\n",
    "# initialize the path.  \n",
    "training_data_path = '../../Week3&4/Data_Part2/Training/brown/' \n",
    "\n",
    "# init a list to contain lists, where each list is a tokenized sentence. \n",
    "corpus_sent = [] \n",
    " \n",
    "# for each file, tokenize the sentence, and \n",
    "for filename in os.listdir(training_data_path):\n",
    "    with open(training_data_path + filename, 'r') as txt_file: \n",
    "        # drop the newline character and don't add empty lists to the corpus list.\n",
    "        for line in txt_file:\n",
    "            temp = line.strip('\\n') \n",
    "            if temp != '':\n",
    "                corpus_sent.append(line.split())\n",
    "print('This is what the first two elements in the list looks like--each list is a tokenized sentence.')\n",
    "corpus_sent[0:2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step 0.b.**   \n",
    "Create a collection for the word sequences and tag sequences. for all sequences, the start and stop tokens are incorporated at the start and end of all sequences, in both collections. This also means, that I am treating the start and stop tokens as two unique tags. I am choosing to do this arbitrarily, but it also keeps the indexes of each tag sequence in line with its associated word sequence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shown is 1 sequence of words and its associated tag sequence (be wary of the commas):\n",
      "\n",
      "['<s>', 'The', 'jury', 'further', 'said', 'in', 'term-end', 'presentments', 'that', 'the', 'City', 'Executive', 'Committee', ',', 'which', 'had', 'over-all', 'charge', 'of', 'the', 'election', ',', '``', 'deserves', 'the', 'praise', 'and', 'thanks', 'of', 'the', 'City', 'of', 'Atlanta', \"''\", 'for', 'the', 'manner', 'in', 'which', 'the', 'election', 'was', 'conducted', '.', '</s>']\n",
      "\n",
      "['<s>', 'at', 'nn', 'rbr', 'vbd', 'in', 'nn', 'nns', 'cs', 'at', 'nn-tl', 'jj-tl', 'nn-tl', ',', 'wdt', 'hvd', 'jj', 'nn', 'in', 'at', 'nn', ',', '``', 'vbz', 'at', 'nn', 'cc', 'nns', 'in', 'at', 'nn-tl', 'in-tl', 'np-tl', \"''\", 'in', 'at', 'nn', 'in', 'wdt', 'at', 'nn', 'bedz', 'vbn', '.', '</s>']\n",
      "==========================================================\n",
      "notice the size of the word_sequences are equal to that of the tag_sequences.\n",
      "length of the word sequences 57841\n",
      "length of the tag sequences 57841\n"
     ]
    }
   ],
   "source": [
    "# create two seperate lists, one for words, and the other for tags. \n",
    "word_sequences = [] \n",
    "tag_sequences = [] \n",
    "\n",
    "for tokenized_sentences in corpus_sent:\n",
    "    word_sequence = ['<s>']\n",
    "    tag_sequence = ['<s>']\n",
    "    \n",
    "    for tagged_word in tokenized_sentences:\n",
    "        # split the tagged word at the '/' \n",
    "        split_tagged_word = tagged_word.split('/')\n",
    "        \n",
    "        # edge case, to handle this exact sequence of chars: ''/''\n",
    "        if split_tagged_word[0] == '': \n",
    "            word_sequence.append('')\n",
    "            tag_sequence.append('')\n",
    "            \n",
    "        # edge case, where a word in the corpus is not tagged: 'ca01' had occured in the corpus with no tag.\n",
    "        elif len(split_tagged_word) < 2: \n",
    "            continue \n",
    "        \n",
    "        # else append the word and tag to the associated collection\n",
    "        else: \n",
    "            word_sequence.append(split_tagged_word[0])\n",
    "            tag_sequence.append(split_tagged_word[1])\n",
    "            \n",
    "    # add the (sequences with a stop token) to their associated collections.\n",
    "    word_sequence.append('</s>')\n",
    "    tag_sequence.append('</s>')\n",
    "    word_sequences.append(word_sequence) \n",
    "    tag_sequences.append(tag_sequence)\n",
    "\n",
    "print('shown is 1 sequence of words and its associated tag sequence (be wary of the commas):\\n')\n",
    "print(word_sequences[1]) \n",
    "print()\n",
    "print(tag_sequences[1])\n",
    "print('==========================================================')\n",
    "\n",
    "print(\"notice the size of the word_sequences are equal to that of the tag_sequences.\")\n",
    "print('length of the word sequences', len(word_sequences))\n",
    "print('length of the tag sequences', len(tag_sequences))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step 1.**  \n",
    "Generate the Tag-word pair counts: denoted C (t_j, w_j):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "this is an example of the word-token frequencies for a specific tag \"at\":\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "defaultdict(<function __main__.<lambda>.<locals>.<lambda>()>,\n",
       "            {'The': 6725,\n",
       "             'an': 3518,\n",
       "             'no': 1575,\n",
       "             'the': 62288,\n",
       "             'a': 21824,\n",
       "             'A': 1119,\n",
       "             'every': 434,\n",
       "             'An': 188,\n",
       "             'Every': 56,\n",
       "             'No': 230,\n",
       "             \"th'\": 1,\n",
       "             \"ever'\": 1})"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "# create dictionary collection of {tag:{word:Freq}}\n",
    "tag_word_pair_counts = defaultdict(lambda: defaultdict(lambda:0)) \n",
    "\n",
    "# I will treat start and stop tokens tags. Also, this is an O(n) iteration, where n is the total number of words in the corpus.\n",
    "for i in range(len(word_sequences)): \n",
    "    for j in range(len(word_sequences[i])): \n",
    "        tag_word_pair_counts[tag_sequences[i][j]][word_sequences[i][j]] += 1\n",
    "\n",
    "print('this is an example of the word-token frequencies for a specific tag \"at\":')\n",
    "tag_word_pair_counts['at']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step 2.**   \n",
    "Generate the Tag-unique counts: denoted C(t_i):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here is an example of a tag frequency value for the tag \"nn\"\n",
      "There are 152393 occurances of the tag \"nn\" in tag_unique counts.\n",
      "\n",
      "There are a total of 525 unique tags in this dictionary.\n"
     ]
    }
   ],
   "source": [
    "tag_unique_counts = defaultdict(lambda: 0) \n",
    "\n",
    "for tag_sequence in tag_sequences: \n",
    "    for tag in tag_sequence: \n",
    "        tag_unique_counts[tag] += 1\n",
    "        \n",
    "print('Here is an example of a tag frequency value for the tag \"nn\"')\n",
    "print('There are {} occurances of the tag \"nn\" in tag_unique counts.'.format(tag_unique_counts['nn']))\n",
    "print()\n",
    "print('There are a total of {} unique tags in this dictionary.'.format(len(tag_unique_counts.keys())))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step 3.**  \n",
    "Generate the Tag-bigram counts: denoted C(T_(i-1), T_i): "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('<s>', 'at'), ('at', 'np-tl'), ('np-tl', 'nn-tl'), ('nn-tl', 'jj-tl'), ('jj-tl', 'nn-tl'), ('nn-tl', 'vbd'), ('vbd', 'nr'), ('nr', 'at'), ('at', 'nn'), ('nn', 'in'), ('in', 'np$'), ('np$', 'jj'), ('jj', 'nn'), ('nn', 'nn'), ('nn', 'vbd'), ('vbd', '``'), ('``', 'at'), ('at', 'nn'), ('nn', \"''\"), (\"''\", 'cs'), ('cs', 'dti'), ('dti', 'nns'), ('nns', 'vbd'), ('vbd', 'nn'), ('nn', '.'), ('.', '</s>')]\n"
     ]
    }
   ],
   "source": [
    "# generate the bigrams for every tag_sequences in all tag_sequences. \n",
    "bigram_sequences = [list(ngrams(tag_sequence, 2)) for tag_sequence in tag_sequences]\n",
    "print(bigram_sequences[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "as we can see, every element in the bigram above is correctly generated based on an existing\n",
      "tag sequence at a given index in the tag_sequences collection.\n",
      "\n",
      "['<s>', 'at', 'np-tl', 'nn-tl', 'jj-tl', 'nn-tl', 'vbd', 'nr', 'at', 'nn', 'in', 'np$', 'jj', 'nn', 'nn', 'vbd', '``', 'at', 'nn', \"''\", 'cs', 'dti', 'nns', 'vbd', 'nn', '.', '</s>']\n"
     ]
    }
   ],
   "source": [
    "# check to ensure that the bigram_sequences[0] correctly corresponds to tag_sequences[0].\n",
    "print('as we can see, every element in the bigram above is correctly generated based on an existing\\n' +\n",
    "      'tag sequence at a given index in the tag_sequences collection.')\n",
    "print()\n",
    "print(tag_sequences[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here is an example of the tag_bigram_counts for the tags that have \"at\" as their preceding tag in a sequence:\n",
      "\n",
      "defaultdict(<function <lambda>.<locals>.<lambda> at 0x000001C21FDDF2F0>, {'np-tl': 809, 'nn': 48368, 'nn-tl': 2564, 'np': 2228, 'jj': 19480, 'jjt': 675, 'ap': 3007, 'nns': 7214, 'nn$': 907, 'vbg': 1568, 'cd': 977, 'jjs': 206, 'vbn': 1468, 'jj-tl': 1412, 'nps': 588, 'od': 1251, '``': 620, 'nns$': 97, 'rb': 350, 'ql': 1377, 'jjs-tl': 2, 'nn$-tl': 162, 'jjr': 630, 'vbn-tl': 390, 'nr-tl': 208, 'nns-tl': 284, 'fw-in': 7, 'abn': 42, 'nr': 218, 'nps$': 30, 'pn': 149, 'nns$-tl': 28, '*': 4, '2': 3, '</s>': 14, 'np$': 62, \"'\": 24, 'vbg-tl': 34, 'od-tl': 98, 'jjr-tl': 3, '2-story': 1, 'fw-nn-tl': 52, 'rb-tl': 1, 'cd-tl': 29, 'fw-jj-tl': 8, 'nr$-tl': 8, 'fw-nn': 76, '2-inch': 3, 'rbt': 11, '(': 15, \"''\": 7, 'cc': 3, 'vb': 16, 'rb-nc': 1, 'vbz': 1, 'rp': 1, 'np$-tl': 17, 'vbd': 2, ',': 10, '--': 4, 'pn$': 1, 'rbr': 40, 'nn+hvz': 3, 'fw-in-tl': 4, 'in': 3, 'nn+bez': 13, \"2''\": 1, \"4''\": 2, \"8''\": 1, 'under': 1, '2-foot': 1, '16-inch': 2, 'md': 1, 'fw-rb': 1, 'nps-tl': 2, 'fw-nns': 11, 'nn+md': 2, 'at': 1, 'ppo': 1, 'fw-jj': 2, 'fw-vb': 2, 'nil': 1, 'jjt-tl': 3, 'fw-nns-tl': 5, 'fw-vbn': 1, 'fw-nn-tl-nc': 1, ')': 1, 'nn-hl': 3, 'vb-tl': 3, 'bez-nc': 1, 'jj-hl': 1, 'fw-jjt': 1, '7074': 1, 'Output': 2, 'nn+in': 1, 'fw-nn$': 1, 'wdt': 2, 'uh': 2, 'ap$': 1, 'fw-cc': 1, 'ap-tl': 2, 'pn+hvz': 1, 'rbr+cs': 1, '.': 2, 'pps': 1, 'in-tl': 1})\n"
     ]
    }
   ],
   "source": [
    "# generate the tag_bigram_counts.\n",
    "tag_bigram_counts = defaultdict(lambda: defaultdict(lambda : 0))\n",
    "\n",
    "for lst_of_bigrams in bigram_sequences: \n",
    "    for bigram in lst_of_bigrams: \n",
    "        tag_bigram_counts[bigram[0]][bigram[1]] += 1\n",
    "        \n",
    "print('Here is an example of the tag_bigram_counts for the tags that have \"at\" as their preceding tag in a sequence:') \n",
    "print()\n",
    "print(tag_bigram_counts['at'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2.2 - Transition Probabilities:  \n",
    "Compute the transition probability of a tag given its previous tag. \n",
    "\n",
    "\\begin{equation*}\n",
    "P(t_i | t_{i-1}) = \\frac{C(t_{i-1}, t_i)}{C(t_{i-1})}\n",
    "\\end{equation*}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Purpose: compute the transition probabilities for a tag given a historical tag occured. \n",
    "def get_transition_prob(tag: str, historical_tag: str) -> float:\n",
    "    # handle edge case, where an arguement is ''.\n",
    "    if tag == '': \n",
    "        tag = \"''\"\n",
    "    if historical_tag == '': \n",
    "        historical_tag = \"''\"\n",
    "    return float(tag_bigram_counts[historical_tag][tag] / tag_unique_counts[historical_tag])  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a precomputed table A, as a dictionary that is stores the transition probs. \n",
    "# the transition prob is accessed by A[tag_(i-1)][tag_i]\n",
    "A = defaultdict(lambda: defaultdict(lambda: 0)) \n",
    "\n",
    "for historical_tag in tag_bigram_counts.keys(): \n",
    "    for tag in tag_bigram_counts[historical_tag].keys(): \n",
    "        A[historical_tag][tag] = get_transition_prob(tag, historical_tag) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2.3 - Emission Probabilies:   \n",
    "Compute the emission probability of a word given a tag.  \n",
    "\n",
    "\\begin{equation*} \n",
    "P(w_i|t_i) = \\frac{C(t_i, w_i}{C(t_i)}\n",
    "\\end{equation*}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_emission_prob(word:str, tag:str) -> float: \n",
    "    # handle edge case, where a tag is ''.\n",
    "    if tag == '': \n",
    "        tag = \"''\"\n",
    "    return float(tag_word_pair_counts[tag][word] / tag_unique_counts[tag])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a precomputed table B, as a dictionary that stores the emission probs. \n",
    "# the emission prob is accesed by B[tag][word]\n",
    "B = defaultdict(lambda: defaultdict(lambda: 0)) \n",
    "\n",
    "for tag in tag_word_pair_counts.keys(): \n",
    "    for word in tag_word_pair_counts[tag].keys(): \n",
    "        B[tag][word] = get_emission_prob(word, tag)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2.4 - Viterbi Algorithm:  \n",
    "for each word in the test set, derive the most probable POS tag sequence using the veterbi algorithm. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here is an example of how the first test sequence looks:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['<s>',\n",
       " 'We',\n",
       " 'have',\n",
       " 'learned',\n",
       " 'much',\n",
       " 'about',\n",
       " 'interstellar',\n",
       " 'drives',\n",
       " 'since',\n",
       " 'a',\n",
       " 'hundred',\n",
       " 'years',\n",
       " 'ago',\n",
       " ';',\n",
       " ';',\n",
       " 'that',\n",
       " 'is',\n",
       " 'all',\n",
       " 'I',\n",
       " 'can',\n",
       " 'tell',\n",
       " 'you',\n",
       " 'about',\n",
       " 'them',\n",
       " '.',\n",
       " '</s>']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# read in test file, and seperate out the sentences into token sequences.\n",
    "\n",
    "import re \n",
    "test_file = '../../Week3&4/Data_Part2/Test/Test_File.txt'\n",
    "test_token_sequences = [] \n",
    "\n",
    "# identifier for the start and end of a sentence\n",
    "regex_start = re.compile(r'< sentence ID =[\\d]')\n",
    "regex_end = re.compile(r'<EOS>')\n",
    "\n",
    "with open(test_file, 'r') as txt_file: \n",
    "    line = (txt_file.readline()).strip('\\n')\n",
    "    line = line.strip('\\n')\n",
    "    \n",
    "    while (line): \n",
    "        \n",
    "        # found the start of a sequence\n",
    "        if regex_start.match(line):\n",
    "            sequence = ['<s>']\n",
    "            line = (txt_file.readline()).strip('\\n')\n",
    "            while (not regex_end.match(line)): \n",
    "                sequence.append(line)\n",
    "                line = (txt_file.readline()).strip('\\n')\n",
    "            # add the stop token to the sequence \n",
    "            sequence.append('</s>') \n",
    "            # add the sequence to all sequences\n",
    "            test_token_sequences.append(sequence) \n",
    "            \n",
    "        line = (txt_file.readline()).strip('\\n') \n",
    "\n",
    "print('Here is an example of how the first test sequence looks:')\n",
    "test_token_sequences[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the sum of the values in the initial probability distribution is:  1.0\n"
     ]
    }
   ],
   "source": [
    "# generate the initial probability distribution. \n",
    "pi = defaultdict(lambda:0.00) \n",
    "\n",
    "# save the total sum of tag frequencies to variable. \n",
    "total_tag_freqs = sum(tag_unique_counts.values())\n",
    "\n",
    "# generate the pi distribution\n",
    "for tag in tag_unique_counts: \n",
    "    pi[tag] = tag_unique_counts[tag]/total_tag_freqs\n",
    "    \n",
    "# check that the sum of the values is equal to 1. \n",
    "print(\"the sum of the values in the initial probability distribution is: \", round(sum(pi.values()), 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# helper functions for the viterbi algorithm \n",
    "import math \n",
    "\n",
    "# purpose: finds which state is most frequence\n",
    "def get_most_frequent_state(pointer, viterbi, competing_state_idx, column) -> tuple:\n",
    "    # get the frequency of occurances for the state the pointer currently holds. \n",
    "    current_state_freq = tag_unique_counts[states[pointer[0]]]\n",
    "    \n",
    "    # get the freq of occ. for the competing state\n",
    "    competing_state_freq = tag_unique_counts[states[competing_state_idx]]\n",
    "    \n",
    "    # compare the frequencies, and return a pointer (tuple) to the most frequent state. \n",
    "    if current_state_freq > competing_state_freq: \n",
    "        return pointer\n",
    "    return (competing_state_idx, column, viterbi[competing_state_idx][column])\n",
    "\n",
    "# purpose: gets the max value from the previous cell\n",
    "# the larger values result in smaller negatives. \n",
    "def find_max_column_state(viterbi:dict, column:int, num_rows:int,  backpointers: list): \n",
    "    # create a pointer to store the row_idx, colum_idx, and value\n",
    "    pointer = (0, column, viterbi[0][column]) \n",
    "    \n",
    "    for state_index in range(1, num_rows): \n",
    "        # if the current max is smaller than the new viterbi value, update the pointer.\n",
    "        if pointer[2] > viterbi[state_index][column]: \n",
    "            pointer = (state_index, column, viterbi[state_index][column]) \n",
    "        \n",
    "        # I make an assumption, that if two values are equal, I will use the more frequent of the two states.\n",
    "        elif pointer[2] == viterbi[state_index][column]:\n",
    "            pointer = get_most_frequent_state(pointer, viterbi, state_index, column)  \n",
    "            \n",
    "    # the max state becomes a part of our path. \n",
    "    backpointers.append(pointer) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The original Implementation of viterbi, resulted in underflow resulting from the following equation used to compute the most probable tage sequence from a bigram tagger: \n",
    "\n",
    "\\begin{equation*}\n",
    "    {\\hat t}{_l^n} = argmax_{t{_l^n}}\\sideset{}{^n}\\prod_{i=1}P(w_i|t_i)P(t_i | t_{i-1})\n",
    "\\end{equation*}  \n",
    "\n",
    "To handle this problem, I transformed my probabilities to log space, which is results in the following: \n",
    "\n",
    "\\begin{equation*}\n",
    "    {\\hat t}{_l^n} = argmax_{{t}{_l^n}}\\sideset{}{^n}\\sum_{i=1}logProb(w_i|t_i) + logProb(t_i | t_{i-1})\n",
    "\\end{equation*}\n",
    "\n",
    "Thus the calculation for a cell from the viterbi lattice went from being in the multiplicative domain: \n",
    "\n",
    "\\begin{equation*}\n",
    "    v_t(j) = max_{i=1}v_{t-1}(i)a_{ij}b_{j}(o_t)\n",
    "\\end{equation*} \n",
    "\n",
    "to the additive domain as follows: \n",
    "\n",
    "\\begin{equation*}\n",
    "    v_t(j) = max_{i=1}v_{t-1}(i) + a_{ij} + b_{j}(o_t)\n",
    "\\end{equation*} "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the unique states globally. \n",
    "states = list(tag_unique_counts.keys())\n",
    "\n",
    "# recall (defined above in 2.2 and 2.3) we are using A as the transition matrix, \n",
    "# and B as the emission matrix. \n",
    "     \n",
    "# define the viterbi algorithm using logprobs: \n",
    "def viterbi(word_sequence:list) -> list: \n",
    "    # init viterbi as a dict of dicts, (its cleaner than using lists,) \n",
    "    # viterbi[row_index][column_index], where the keys are int values. \n",
    "    vit_lattice = defaultdict(lambda: defaultdict(lambda:0.00))\n",
    "    \n",
    "    # init the backpointer sequence as a list containing tuple(state, observation, value)  \n",
    "    backpointers = [] \n",
    "    \n",
    "    # init the stopping bounds of the matrix \n",
    "    num_states = len(states)   \n",
    "    num_observations = len(word_sequence) \n",
    "    \n",
    "    # generate the first column. \n",
    "    for state_index in range(num_states): \n",
    "        \n",
    "        # precompute the initial transition and emission prob.\n",
    "        pi_state = pi[states[state_index]]\n",
    "        emission_prob = B[states[state_index]][word_sequence[0]]\n",
    "        \n",
    "        # if the pi or emission state is 0, just set the value to 0 becuase it can't be mapped to log space, \n",
    "        # and in the multiplicitave space the lattice value would have resulted in 0 as well. \n",
    "        if pi_state == 0 or emission_prob == 0: \n",
    "            vit_lattice[state_index][0] = 0 \n",
    "        else: \n",
    "            vit_lattice[state_index][0] = math.log(pi_state) + math.log(emission_prob) \n",
    "        \n",
    "    # use the helper functions to update the backpointer, i.e., add the 1st element in the tag sequence  \n",
    "    find_max_column_state(vit_lattice, 0, num_states, backpointers)\n",
    "    \n",
    "    # generate the remaining columns in the tag sequence\n",
    "    for column_index in range(1, num_observations): \n",
    "        for state_index in range(num_states): \n",
    "            \n",
    "            # get the previous columns max value, and the associated state. \n",
    "            prev_val = backpointers[column_index - 1][2] \n",
    "            prev_tag = states[backpointers[column_index - 1][0]]\n",
    "            \n",
    "            # identify if any of these values are 0 before converting to log space\n",
    "            transition_prob = A[prev_tag][states[state_index]]\n",
    "            emission_prob = B[states[state_index]][word_sequence[column_index]]\n",
    "            if transition_prob == 0 or emission_prob == 0: \n",
    "                vit_lattice[state_index][column_index] = 0 \n",
    "            else: \n",
    "                # compute the new column values = prev_val + A[t_i][t_i-1] + B[tag][word]\n",
    "                vit_lattice[state_index][column_index] = (prev_val + \n",
    "                                                         math.log(transition_prob) + \n",
    "                                                         math.log(emission_prob))\n",
    "            \n",
    "        # use the helper functions to update the backpointer, i.e., add the next element in the tag sequence  \n",
    "        find_max_column_state(vit_lattice, column_index, num_states, backpointers)            \n",
    "    \n",
    "    return backpointers "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use the global 'states' variable, and local test_token_sequence[i] to generate a list of the (tag:word) sequence\n",
    "def generate_tag_sequence(backpointers: list, word_sequence: list) -> list: \n",
    "    tag_seq = [] \n",
    "    for pointer in backpointers:\n",
    "        state = states[pointer[0]]\n",
    "        observation = word_sequence[pointer[1]] \n",
    "        tag_seq.append((observation, state)) \n",
    "    return tag_seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('<s>', '<s>'),\n",
       " ('We', 'ppss-hl'),\n",
       " ('have', 'hv-hl'),\n",
       " ('learned', 'nn'),\n",
       " ('much', 'ap'),\n",
       " ('about', 'rp'),\n",
       " ('interstellar', 'jj'),\n",
       " ('drives', 'vbz'),\n",
       " ('since', 'rb'),\n",
       " ('a', 'at-tl'),\n",
       " ('hundred', 'cd'),\n",
       " ('years', 'nns'),\n",
       " ('ago', 'rb'),\n",
       " (';', '.'),\n",
       " (';', '.'),\n",
       " ('that', 'nn'),\n",
       " ('is', 'nil'),\n",
       " ('all', 'nn'),\n",
       " ('I', 'nn-tl'),\n",
       " ('can', 'vb'),\n",
       " ('tell', 'vb-nc'),\n",
       " ('you', 'ppo'),\n",
       " ('about', 'rp'),\n",
       " ('them', 'dts'),\n",
       " ('.', '.'),\n",
       " ('</s>', '</s>')]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# test the viterbi algo on one sequence, and generate the (word, tag) outcome. \n",
    "l = viterbi(test_token_sequences[0])\n",
    "generate_tag_sequence(l, test_token_sequences[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Execute the viterbi algorithm on the test set, and generate a collection containing the (word,tag) sequences. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a collection of (word, tag) tuple sequences \n",
    "resulting_sequences = [] \n",
    "\n",
    "# generate the (word, tag) sequences by using \n",
    "# viterbi and mapping the backpointers to the word and tag. \n",
    "for word_sequence in test_token_sequences: \n",
    "    backpointers = viterbi(word_sequence) \n",
    "    resulting_sequences.append(generate_tag_sequence(backpointers, word_sequence))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HMM_predictions.txt has been generated.\n"
     ]
    }
   ],
   "source": [
    "# write to the results to a file: \n",
    "file = open('HMM_predictions.txt', \"w\") \n",
    "for sequence_num, tupls in enumerate(resulting_sequences, start=1): \n",
    "    # start of a sentence sequence. \n",
    "    file.write('<sentence ID=' + str(sequence_num) + '>\\n')\n",
    "    \n",
    "    # body of a sequence, ignore ('<s>', '<s>') and ('</s>', '</s>').\n",
    "    sequence_stop_val = len(tupls)-1\n",
    "    for i in range(1, sequence_stop_val): \n",
    "        file.write(tupls[i][0] + ', ' + tupls[i][1]+'\\n')\n",
    "    \n",
    "    # end the sequence. \n",
    "    file.write('<EOS>\\n') \n",
    "    \n",
    "file.close() \n",
    "print('HMM_predictions.txt has been generated.') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclusion:   \n",
    "Using the procedure outlined through steps 2.1 to 2.4, I was able to generate the needed components of an HNN model to generate the viterbi lattice, and ultimitely the tag sequence prediction for a given sentence.\n",
    "\n",
    "As seen in this notebook, I used a transition probability matrix **A**, an observation matrix containing emission probabilites **B**, a set of N states defined by the **states** variable, a sequence of observations stored in a collection of sequences **test_token_sequences**, and a collection of initial probability distributions **pi**.  \n",
    "\n",
    "When generating the viterbi lattice, I originally had underflow problems because I was in the multiplicative domain. To circumvent this, I mapped my **initial probability distributions**, **transition probabilities**, and **emission states** to the multiplicitave domain. \n",
    "\n",
    "The resulting tags for the test set was written to the 'HMM_predictions.txt' file. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
